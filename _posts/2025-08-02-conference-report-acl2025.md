---
title: 'Conference Report ACL 2025 - How can we move forward as a field?'
date: 2025-08-02
permalink: /posts/2025-08-02
tags:
---

<i>DISCLAIMER: If you disagree with me, or if I represented your opinions wrongly in this article, please contact me directly via e-mail! I am happy to adapt this article if needed.</i>

Last week, I attended the 63rd Annual Meeting of the Association for Computational Linguistics (ACL 2025). My PhD advisor Manfred Pinkal told me recently that the first ACL he attended featured 150 attendees. This year, there were more than 5000 onsite and roughly 1000 additional remote participants. It was also the first time I attended a huge conference after the Covid break, so it was extra nice to catch up with a lot of people in person.

Every invited talk and panel (except maybe that by Mark Johnson in the XLLM workshop) that I attended spent the first ten minutes discussing the great capabilities of large language models (LLMs) and why they do not yet solve language understanding, or some related tasks. Moreover, ACL ARR has a massive problem with way too many submissions, too few senior reviewers, and too many automatically generated reviews. In a plenary <a href="https://2025.aclweb.org/program/panel/">panel discussion</a>, Ed Hovy, Mirella Lapata, Yue Zhang, and Dan Roth discussed how we could move forward as a field.

* <strong>Ed:</strong> With LLMs, we are given a self-driving car that just crasheds regularly. We should spend more effort on understanding how it works internally. I think this is really worthwhile, though he really meant that we should develop methods that let us understand the internal workings of the model such that we can fix 'misbehavior'. All the papers that just probe the models on some task and then say, hey, LLMs can do this or that (to some degree) are like popcorn (his words, not mine), you read them, you feel satisfied for ten minutes, and then you feel empty again. I chatted with Ed later and (if I did not misunderstand him) he actually does agree that system-building using LLMs as a component is still a worthwhile endeavor, if that's your focus. I will come back to this point later. As a community, we have tried understanding how neural models work for quite a while, e.g., along the lines of the <a href="https://blackboxnlp.github.io/2025/">BlackBoxNLP workshop series</a>, which is still active. There were some interesting insights, e.g., that <a href="https://aclanthology.org/P19-1452/">BERT behaves a bit like the old natural language processing (NLP) pipelines</a> with higher-level semantics being put together at higher levels. However, I think we have failed to even understand simple models like word2vec fully in the sense of being able to predict when they succeed and when they fail. In my current opinion (which I am happy to change if someone brings evidence or good arguments), LLMs are still distributional models, with just highly sophisticated vector spaces that enable us to store a lot of information how to traverse the vector space and output natural language answers at the end. It does make sense to look at where they succeed and where they fail. In particular when building systems for real-world use, it is paramount to test them extensively and make sure they do not harm anyone. Unfortunately, the popcorn papers are overwritten as soon as the next model generation comes out. Then we have to test everything again, and unless we can be sure that the new models were not simply trained on the probing test data, we even cannot re-use the same benchmark data. We've already seen this with all the BERTology papers.

* <strong>Mirella:</strong> We need to get back to an actual science, where we have a controlled setting of what went into the training data and on what data we test. I completely agree. This was mainly a call to the model vendors. The issue is that indexing and providing search capabilities for the pretraining data is of course costly. Plus, we would have to keep developing methods to find the instances of data that are of interest to the model behavior we aim to analyze, which may also raise new questions how to do this science. (Though I personally think doing research on such retrieval methods could be fun.) During the XLLM workshop, I learned about some interesting endeavors in this direction like <a href="https://openeurollm.eu/">OpenEuroLLM</a>. However, for most of use, regular researchers at a regular unversity, training such models is not something we can actually afford. So what can we do to make the field more scientific again? I will present my thoughts on that below.

* <strong>Dan:</strong> The topic of the panel discussion was actually supposed to be "generalisation," even if a large part of the discussion was more general than that (pun intended). Dan reported that he had asked some LLM about himself recently and got an impressive list of prizes he had won, just that he had not actually won them. But he pointed out that the model actually <i>did</i> generalize by hallucinating these facts, as they typically fit for a university professor at his stage. If I understood correctly, Dan, who was presented as the "pragmatist," largely took the perspective that we should build and evaluate systems. But also that we should aim to build systems (or understand LLMs in that sense) how they deal with reasoning chains or, as they put it, causal reasoning. In our own work on <a href="https://aclanthology.org/2024.emnlp-main.153/">quantifying uncertainty in natural language text in Bayesian reasoning scenarios</a> (EMNLP 2024), we actually found that they perform okay on causal reasoning (inferring the likelihood of effects based on some cause), but that their performance drops markedly when the underlying problem requires evidential (updating one's beliefs about causes when new evidence comes in) or explaining-away style reasoning (in which the knowledge about one cause makes the beliefs that some other cause is the case less likely in light of evidence). This, I think, is actually somewhat intuitive given their autoregressive nature.

<img src="https://github.com/annefried/annefried.github.io/blob/master/images/quite-results.png?raw=true" alt="Results taken from our EMNLP 2024 paper: the models' performance drops for evidential and explaining-away reasoning"/>

What is worth noting is that despite the teaser image, our paper is not actually popcorn (I hope), because our point was actually to create a neurosymbolic model that parses problems into a machine-readable logic programming language that can then solve the problems regardless of the underlying reasoning types (see blue bars in the plot). Results like this make me somehow believe that without a major change in model architecture, the models have some built-in bias that just works unlike the human brain, and that to achieve models that explain reasoning in a way that we humans can deal with, we need at least one additional architecture shift in AI. But that is a belief, and as a researcher, I occasionally update my beliefs. Evidential, causal, or explaining-away reasoning included.

So still, ACL has a massive problem of somehow having drifted away from the scientific principles guiding NLP research in the past decades. Getting papers accepted seems to have become some kind of gamble which highly depends on being assigned a responsible meta-reviewer. In his keynote at the Scientific Document Processing Workshop, Ed Hovy again provocatively stated that <a href="https://aclanthology.org/2024.acl-long.547/">automatic meta-reviewing had been solved</a> (don't get me wrong, I loved his provocative analogies, our community needs leaders like him that have thought about NLP and meaning for decades). Again, in our coffee break chat, he agreed that neiter reviewing nor meta-reviewing should be summarization. Practically, the meta-reviews that my students receive regularly read like superficial summaries of the reviews. There is no meaningful evaluation of the contributions as in prior times. But it shouldn't be that way. Ed actually thinks we should admit all the papers and vote on-site which papers should get into the proceedings. A litte like it is actually the case in linguistics - in this field, contributions are often accepted for presentation at a conference based on an abstract, to check the topical fit. After discussion and feedback, editors that actually edit compile a collection of articles into a book. Maybe that wouldn't be a bad idea, I am just not sure whether it scales with the number of ACL participants. But a lot of people I talked to at ACL this year (including also Alexander Koller) and also ACL president Chengqing Zong in his presidential address advocated going back to smaller and specialized conferences.

What bothers me, personally, is <strong>understanding how we should do science these days</strong>. And how to communicate this to the newcomers in our field. On my train ride back home, I was thinking about how I communicate this to my PhD student and decided to put together some concrete suggestions that can help people who do not own huge compute centers. But first, let's look back. What were experimental setups, what were "interesting" or "valid" contributions?

## Computational Linguistics before the year 2000

For decades, computational linguistics dealt with building programs (typically not yet what we would call a system today) that would process natural language, often motivated by theoretically established linguistic rules. The contribution was as much on the linguistic side as on the computational side, as by formalizing and testing linguistic ideas in a computational way was still rather novel. For example, look at the <a href="https://aclanthology.org/P87-1003.pdf">PUNDIT system</a> for temporal relation inference (which is more on the theoretical side) or a <a href="https://aclanthology.org/M91-1023.pdf">Text Interpretation System</a> for MUC-3.

<img src="https://github.com/annefried/annefried.github.io/blob/master/images/examples-cl-1990.png?raw=true" alt="screenshots taken from the two papers mentioned in the text that illustrate how the grammar rules look like" />

These papers do not even have an experimental or evaluation section. <strong>Reviewers would have to look at the ideas and judge whether they would be interesting to discuss, whether they have the potential to spark new ideas in others.</strong> Whether the approach does something different than existing work. <strong>Proposal #1</strong>: Let's add this back to our criteria of reviewing and meta-reviewing, and not just on guidelines pages, but actually do that. It will result in much more interesting contributions to ACL than benchmark chasing. I think this fits in nicely with the necessity to create less compute-intensive models. And let's not just do that in specific environmental computing tracks. Let's invite diversity back into our approaches.

To illustrate what is currently wrong: The only paper from my group that did not get into ACL or Findings actually presented a super interesting way to create training data for a complicated semantic parsing task and showed clear improvements on medium-sized models. It was rejected mainly for the reason that GPT4o and Deepseek achieved around 61% accuracy on the dataset, and our approach only 57%. Are these really numbers that already tell us that we are on the wrong track? What if our model had achieved 63% accuracy? I think if any SOTA LLMs achieve anything less than 98% accuracy on a tasks, it is absolutely worth discussing alternative approaches! (This paragraph is not intended to be a personal complaint, it just intends to illustrate what I also hear from lots of colleagues, despite the ACL ARR guidelines already discouraging such simplified ways of evaluating the perceived utility of an idea.)

## Natural Language Processing 2000-2022

In the 2000s, the mainstream paradigm in NLP was to collect data, annotate it manually, compare whether the task is meaningful by at least checking if several humans would arrive at the same conclusion if they perform it, then train a machine learning model guided by their intuition and a development set not included in the training data. Finally, we would test whether the model generalizes either in- or cross-domain by applying it to some also manually-labeled test data. One could argue that often, the data sets were way too small for drawing statistically significant conclusions (although they were often much bigger than nowadays). Often, it can also be shown that the <a href="https://aclanthology.org/N18-2017/">models actually use superficial cues rather than actual understanding</a>. So I really do not want to say "everything used to be better".
However, it was paramount to separate the data on which the model was trained from the data on which hypotheses were tested. Also, the data should either have been elicited throug human annotation (more often the case, example: <a href="https://aclanthology.org/P16-1166/">my own PhD work on linguistic aspect</a>) or they should have come from the real world (less often the case, examples from the patent NLP domain: <a href="https://aclanthology.org/2022.emnlp-main.791/">a dataset on patent classification created by a Bosch employee over the course of 25 years</a> or <a href="https://aclanthology.org/2025.findings-acl.496/">PAP2PAT, which aligns patents and papers</a>).

<img src="https://github.com/annefried/annefried.github.io/blob/master/images/experimental-paradigm-nlp-2000.png?raw=true" alt="image summarizing the experimental paradigm 2000-2022 that is also explained in the main text"/>

As a side note, it was also considered bad practice to <strong>reverse-engineer</strong> other systems. For example, if you take an existing dependency parser and run it on a dataset, which you then use to train another parser, you have not actually learned anything new, you have just learned the set of rules already explicitly or implicitly incorporated in the initial parser. The only admissible setting that I can think of is when the aim is to construct (typically additional) <i>silver data</i> that is combined with existing <i>gold data</i> and the aim is not to make statements about the underlying phenomenon that you are trying to capture but instead to improve downstream performance on an unseen test set. (I am mentioning this because I noted recently that a lot of novice researchers have not heard of the concept of silver standard data, although much of the currently LLM-created benchmark data falls in this category in my opinion. It was also suggested by Jessy Li in her keynote at the Linguistic Annotation Workshop that we should increasingly work with <i>platinum data</i>, i.e., manually created data collected by actual expert annotators.)

But what can we learn from this age? <strong>Only if the training data does not contain the test set, we can actually make statements about whether a model has learned anything beyond memorizing some instances.</strong> But how can we tell what data LLMs have seen during pre-training? Sadly, at the moment, for the most widely-used LLMs, we cannot. But we can do something. <strong>Proposal #2: We can create test data that was created newly, either by hand-labeling data (by making use of trustworthy annotations, perhaps even having them onsite -- I would not trust crowd-sourcing any more) or by collecting real-world data that was definitely created after the cut-off date of the pretraining data of the LLMs that are part of the system we are testing.</strong> An analysis of such non-contaminated test data should be a mandatory part of any paper using the standard machine-learning benchmarking paradigm described above. And reviewers should give credit for it, because it is a lot of extra work!

Here is an example taken from the PAP2PAT paper. I really liked that my student Valentin collected this additional data and performed experiments on it, actually observing some interesting deviations on the linguistic style, and I think it would be very worth diving into such observations with an even deeper analysis. (This is not criticism of his careful work, which took more than one year, and which I, in my completely unbiased opinion, think it would absolutely have been worthy of being accepted at the main conference. :)) Another paper that I really enjoyed reading from this perspective is <a href="https://aclanthology.org/2023.emnlp-main.335/">Navigating the Grey Area</a> by Kaitlyn Zhou, Dan Jurafsky, and Tatsunori Hashimoto (EMNLP 2023). They inspected the Pile, a common pretraining dataset, to understand why the model exhibits a particular behavior in answering questions when the prompt contains expressions of (un)certainty.

<img src="https://github.com/annefried/annefried.github.io/blob/master/images/pap2pat-example.png?raw=true" alt="a screenshot of the Pat2Pat paper sections on test data contamination"/>


## Reflection on machine learning practices today

<i>to be added</i>

## Looking sideways: Methods in HCI

<i>to be added</i>

## Summary: how can we move forwards?
<i>to be added</i>


<strong>Is there AI-generated content in this article?</strong> I did not use any AI to write this article. Like Iryna Gurevych, who pointed this out in response to Ed's talk at SDProc, I still find it easier and faster to directly find the words that I can use to express something rather than finding the prompt that generates the text that I actually want. Maybe that is because I was actually trained extensively in writing, and my way of doing things is old-fashioned. Like someone who still remembers landline numbers rather than searching for a phone number in their Google contacts. Sometimes the old way is just faster. (I have to admit that I have to look up even my husband's cell number these days, though.) I do not know whether this means that we should be worried as a generation of researchers and developers that learn neither to write nor to program themselves is in a critical education stage these days. Maybe I am really just old-fashioned. On the other hand, how could I learn to write prompts for a translation system and judge if the output is what I want if I do not actually speak the target language?